{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '..\\\\..\\\\Dataset\\\\'\n",
    "dataset_group_exp_path = '..\\\\..\\\\Dataset\\\\group_experiment\\\\'\n",
    "\n",
    "def load_files(case): # 1 : slow, 2 : bottleneck, 3 : fast\n",
    "    if case == 1:\n",
    "        baslar_file = dataset_group_exp_path + 'first.mp4'\n",
    "        bag_file = dataset_group_exp_path + 'first.bag'\n",
    "    elif case == 2:\n",
    "        baslar_file = dataset_group_exp_path + 'second.mp4'\n",
    "        bag_file = dataset_group_exp_path + 'second.bag'\n",
    "    elif case == 3:\n",
    "        baslar_file = dataset_group_exp_path + 'third.mp4'\n",
    "        bag_file = dataset_group_exp_path + 'third.bag'\n",
    "    return baslar_file, bag_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Realsense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bag_file(file_name, real_time=False):\n",
    "    pipe = rs.pipeline()\n",
    "    cfg = rs.config()\n",
    "    cfg.enable_device_from_file(file_name, repeat_playback=False)\n",
    "    profile = pipe.start(cfg)\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(real_time) # False: no frame drop\n",
    "    \n",
    "    # Get the frame shape of the color sensor\n",
    "    frames = pipe.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    frame_shape = (depth_frame.get_height(), depth_frame.get_width())\n",
    "\n",
    "    return pipe, cfg, profile, playback, frame_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_depth_frame(depth_frame, min_distance=0, max_distance=1.5, decimation_magnitude = 1.0, spatial_magnitude = 2.0, spatial_smooth_alpha = 0.5, spatial_smooth_delta = 20, temporal_smooth_alpha = 0.4, temporal_smooth_delta = 20):\n",
    "    # Post processing possible only on the depth_frame\n",
    "    assert (depth_frame.is_depth_frame())\n",
    "\n",
    "    # Available filters\n",
    "    decimation_filter = rs.decimation_filter()\n",
    "    threshold_filter = rs.threshold_filter()\n",
    "    depth_to_disparity = rs.disparity_transform(True)\n",
    "    spatial_filter = rs.spatial_filter()\n",
    "    temporal_filter = rs.temporal_filter()\n",
    "    disparity_to_depth = rs.disparity_transform(False)\n",
    "    hole_filling = rs.hole_filling_filter(1) # https://intelrealsense.github.io/librealsense/doxygen/classrs2_1_1hole__filling__filter.html\n",
    "\n",
    "    # Apply the control parameters for the filters\n",
    "    decimation_filter.set_option(rs.option.filter_magnitude, decimation_magnitude)\n",
    "    threshold_filter.set_option(rs.option.min_distance, min_distance)\n",
    "    threshold_filter.set_option(rs.option.max_distance, max_distance)\n",
    "    spatial_filter.set_option(rs.option.filter_magnitude, spatial_magnitude)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_alpha, spatial_smooth_alpha)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_delta, spatial_smooth_delta)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_alpha, temporal_smooth_alpha)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_delta, temporal_smooth_delta)\n",
    "\n",
    "    # Apply the filters\n",
    "    # Post processing order : https://dev.intelrealsense.com/docs/post-processing-filters\n",
    "    # Depth Frame >> Decimation Filter >> Depth2Disparity Transform >> Spatial Filter >> Temporal Filter >> Disparity2Depth Transform >> Hole Filling Filter >> Filtered Depth\n",
    "    filtered_frame = decimation_filter.process(depth_frame)\n",
    "    filtered_frame = threshold_filter.process(filtered_frame)\n",
    "    filtered_frame = depth_to_disparity.process(filtered_frame)\n",
    "    filtered_frame = spatial_filter.process(filtered_frame)\n",
    "    filtered_frame = temporal_filter.process(filtered_frame)\n",
    "    filtered_frame = disparity_to_depth.process(filtered_frame)\n",
    "    filtered_frame = hole_filling.process(filtered_frame)\n",
    "    \n",
    "    # Cast to depth_frame so that we can use the get_distance method afterwards\n",
    "    depth_frame_filtered = filtered_frame.as_depth_frame()\n",
    "\n",
    "    return depth_frame_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "def process_frames(frames, post_process = True):\n",
    "    # Align the depth frame to color frame\n",
    "    aligned_frames = align.process(frames)\n",
    "\n",
    "    # Get aligned frames\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "    # Validate that both frames are valid\n",
    "    if not depth_frame or not color_frame:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Post process is not included in the BAG file, so we need to apply it\n",
    "    if post_process:\n",
    "        depth_frame = post_process_depth_frame(depth_frame)\n",
    "\n",
    "    # Colorize the depth frame\n",
    "    depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "    # Convert frames to images\n",
    "    depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    return color_frame, depth_frame, color_image, depth_color_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration:  0:00:43.503743\n",
      "Frame shape:  (480, 848)\n"
     ]
    }
   ],
   "source": [
    "# Read the bag file\n",
    "baslar_file, bag_file = load_files(1)\n",
    "bag_file = dataset_path + 'me_and_sasa.bag'\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "duration = playback.get_duration()\n",
    "print(\"Video duration: \", duration)\n",
    "print(\"Frame shape: \", frame_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames:  62\n"
     ]
    }
   ],
   "source": [
    "# Read the full stream\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames, True)\n",
    "\n",
    "        # Verify that the frames are valid\n",
    "        if color_frame is None or depth_frame is None:\n",
    "            continue\n",
    "        \n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        cv2.imshow(\"Depth Image\", depth_color_image)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite('color_image.png', color_image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract height of the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a mask to keep ony the ROI, under the camera, to have a better height estimation\n",
    "# mask = np.zeros(frame_shape, dtype=np.uint8)\n",
    "# center_y, center_x = frame_shape[0] // 2, frame_shape[1] // 2\n",
    "\n",
    "# # Mask size\n",
    "# third_height = frame_shape[0] // 3\n",
    "# top_left = (0, third_height)\n",
    "# bottom_right = (848, 480)\n",
    "\n",
    "# cv2.rectangle(mask, top_left, bottom_right, 255, thickness=-1)\n",
    "\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the aruco detector\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_ARUCO_MIP_36h12)\n",
    "aruco_params = cv2.aruco.DetectorParameters()\n",
    "\n",
    "aruco_params.errorCorrectionRate = 0.2\n",
    "aruco_params.polygonalApproxAccuracyRate = 0.05\n",
    "# aruco_params.minMarkerPerimeterRate = 0.01\n",
    "\n",
    "detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_height = 2.568 # meters, ground truth\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames)\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "        cv2.aruco.drawDetectedMarkers(depth_color_image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Display the height with an offset to avoid overlap\n",
    "            text_position_y = 30 + k * 40\n",
    "            cv2.putText(depth_color_image, \"ID: {} Height: {:.2f}m\".format(id, height), (10, text_position_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        cv2.imshow(\"Output\", depth_color_image)\n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press s to save the frame\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite(\"..\\\\images\\\\color_images\\\\images\\\\color_frame_\" + str(num_frames) + \".png\", color_image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream    \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(data, m=2):\n",
    "    # Z-score method\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    filtered_data = [x for x in data if abs(x - mean) / std < m]\n",
    "    return filtered_data\n",
    "\n",
    "def calculate_mean_height(heights):\n",
    "    mean_heights = {}\n",
    "\n",
    "    for key, height_list in heights.items():\n",
    "        filtered_heights = remove_outliers(height_list)\n",
    "        if filtered_heights:\n",
    "            mean_heights[key] = np.mean(filtered_heights)\n",
    "\n",
    "    return mean_heights\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection of the point on the floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching points on the floor, 130cm and 170cm\n",
    "points_floor = np.array([\n",
    "    [1042, 737, 0],\n",
    "    [1161, 747, 0],\n",
    "    [1287, 760, 0],\n",
    "    [1405, 771, 0],\n",
    "    [1025, 861, 0],\n",
    "    [1148, 870, 0],\n",
    "    [1274, 882, 0],\n",
    "    [1397, 891, 0],\n",
    "    [1011, 986, 0],\n",
    "    [1138, 997, 0],\n",
    "    [1265, 1007, 0],\n",
    "    [1392, 1018, 0],\n",
    "    [996, 1111, 0],\n",
    "    [1128, 1120, 0],\n",
    "    [1257, 1128, 0],\n",
    "    [1382, 1134, 0]\n",
    "])\n",
    "\n",
    "points_130 = np.array([\n",
    "    [805, 395, 130],\n",
    "    [1060, 420, 130],\n",
    "    [1320, 449, 130],\n",
    "    [1554, 473, 130],\n",
    "    [777, 651, 130],\n",
    "    [1040, 677, 130],\n",
    "    [1301, 702, 130],\n",
    "    [1549, 731, 130],\n",
    "    [746, 914, 130],\n",
    "    [1024, 937, 130],\n",
    "    [1286, 960, 130],\n",
    "    [1532, 976, 130],\n",
    "    [722, 1130, 130],\n",
    "    [1017, 1178, 130],\n",
    "    [1257, 1179, 130],\n",
    "    [1507, 1208, 130]\n",
    "])\n",
    "\n",
    "points_170 = np.array([\n",
    "    [598, 91, 170],\n",
    "    [970, 118, 170],\n",
    "    [1346, 167, 170],\n",
    "    [1693, 206, 170],\n",
    "    [537, 445, 170],\n",
    "    [931, 489, 170],\n",
    "    [1323, 536, 170],\n",
    "    [1685, 576, 170],\n",
    "    [481, 842, 170],\n",
    "    [906, 873, 170],\n",
    "    [1301, 916, 170],\n",
    "    [1664, 933, 170],\n",
    "    [444, 1172, 170],\n",
    "    [895, 1240, 170],\n",
    "    [1261, 1241, 170],\n",
    "    [1634, 1278, 170]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_file = dataset_path + 'SINGLE\\\\Image__2024-03-20__16-55-35.png'\n",
    "image = cv2.imread(points_floor_file)\n",
    "\n",
    "for pf, p130, p170 in zip(points_floor, points_130, points_170):\n",
    "    cv2.line(image, pf[:2], p130[:2], (255, 0, 0), 1)\n",
    "    cv2.line(image, p130[:2], p170[:2], (0, 255, 0), 1)\n",
    "\n",
    "cv2.namedWindow('Projection Lines', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Lines', image)\n",
    "cv2.resizeWindow('Projection Lines', 864, 648)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 3 homographies\n",
    "points_floor_2d = points_floor[:, :2]\n",
    "points_130_2d = points_130[:, :2]\n",
    "points_170_2d = points_170[:, :2]\n",
    "\n",
    "H_0, _ = cv2.findHomography(points_floor_2d, points_floor_2d) # Identity matrix (hopefully :))\n",
    "H_130, _ = cv2.findHomography(points_130_2d, points_floor_2d)\n",
    "H_170, _ = cv2.findHomography(points_170_2d, points_floor_2d)\n",
    "\n",
    "# Interpolate the homography for a given height, between 0 and 210 cm\n",
    "def interpolate_homography(z, H_0, H_130, H_170):\n",
    "    if z <= 130: # linear interpolation between 0 and 130\n",
    "        alpha = z / 130\n",
    "        return (1 - alpha) * H_0 + alpha * H_130\n",
    "    \n",
    "    elif z <= 170: # linear interpolation between 130 and 170\n",
    "        alpha = (z - 130) / 40 # /40 to normalize the alpha value (130 < z <= 170)\n",
    "        return (1 - alpha) * H_130 + alpha * H_170\n",
    "    \n",
    "    else: # extrapolation between 170 and 210\n",
    "        # Basically, from 170 cm, the transformation continues to change in the same way as it did between 130 cm and 170 cm\n",
    "        H_210 =  H_170 + (H_170 - H_130)\n",
    "        alpha = (z - 170) / 40 # /40 to normalize the alpha value (170 < z <= 210)\n",
    "        return (1 - alpha) * H_170 + alpha * H_210\n",
    "\n",
    "# Project a point on the floor, given its 3D coordinates\n",
    "def project_point(point, z):\n",
    "    H = interpolate_homography(z, H_0, H_130, H_170)\n",
    "    point_homogeneous = np.hstack([point, 1])\n",
    "    projected_point_homogeneous = H @ point_homogeneous\n",
    "    projected_point = projected_point_homogeneous[:2] / projected_point_homogeneous[2]\n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example to test the homography interpolation\n",
    "image = cv2.imread(points_floor_file)\n",
    "\n",
    "def project_points(points, H):\n",
    "    points_homogeneous = np.hstack([points[:, :2], np.ones((points.shape[0], 1))])\n",
    "    projected_points_homogeneous = (H @ points_homogeneous.T).T\n",
    "    projected_points = projected_points_homogeneous[:, :2] / projected_points_homogeneous[:, 2][:, np.newaxis]\n",
    "    return projected_points\n",
    "\n",
    "cv2.namedWindow('Projection Points', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Points', image)\n",
    "cv2.resizeWindow('Projection Points', 864, 648)\n",
    "\n",
    "# Draw the points at 170 cm\n",
    "for point in points_170.astype(int):\n",
    "    cv2.circle(image, tuple(point[:2]), 10, (255, 0, 0), -1)\n",
    "\n",
    "def on_trackbar(z):\n",
    "    z = int(z)\n",
    "    H = interpolate_homography(z, H_0, H_130, H_170)\n",
    "    projected_points = project_points(points_170, H)\n",
    "    image_with_points = image.copy()\n",
    "    for point in projected_points.astype(int):\n",
    "        cv2.circle(image_with_points, tuple(point), 10, (0, 0, 255), -1)\n",
    "    cv2.imshow('Projection Points', image_with_points)\n",
    "\n",
    "cv2.createTrackbar('Height', 'Projection Points', 170, 210, on_trackbar)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "baslar_file, bag_file = load_files(1)\n",
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "height = 175\n",
    "projected_points = []\n",
    "\n",
    "kernel = np.ones((5, 5), np.uint8)\n",
    "aruco_ids = set()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(image)\n",
    "        cv2.aruco.drawDetectedMarkers(image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            aruco_ids.add(id)\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            projected_point = project_point(point, height)\n",
    "            projected_points.append(projected_point)\n",
    "\n",
    "            cv2.circle(image, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for projected_point in projected_points:\n",
    "            point = projected_point.astype(int)\n",
    "            cv2.circle(image, tuple(point), 10, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', image)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite(\"..\\\\images\\\\color_images\\\\images\\\\baslar_frame_\" + str(num_frames) + \".png\", image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate exit times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baslar_file, bag_file = load_files(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimate heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[19], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m color_frame, depth_frame, color_image, depth_color_image \u001b[38;5;241m=\u001b[39m process_frames(frames)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;66;03m# Detect the aruco markers\u001b[39;00m\n\u001b[1;32m---> 18\u001b[0m corners, ids, rejected \u001b[38;5;241m=\u001b[39m \u001b[43mdetector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetectMarkers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolor_image\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(corners)):\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;28mid\u001b[39m \u001b[38;5;241m=\u001b[39m ids[k][\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "camera_height = 2.568 # meters, ground truth\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames)\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: 1.75, 17: 1.75, 87: 1.75, 24: 1.75, 25: 1.75}\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded heights because the realsense camera was not properly placed\n",
    "aruco_ids = {33, 17, 87, 24, 25}\n",
    "mean_heights = {key: 1.75 for key in aruco_ids}\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "num_frames = 0\n",
    "\n",
    "# Dictionary to store the projected points for each aruco marker id, and the color for each id, only for visualization\n",
    "projected_points = {}\n",
    "color_points = {}\n",
    "for id, _ in mean_heights.items():\n",
    "    color_points[id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "# Data structure to store the enter and exit frame for each aruco marker id\n",
    "enter_frame = {}\n",
    "exit_frame = {}\n",
    "for id, _ in mean_heights.items():\n",
    "    enter_frame[id] = -1\n",
    "    exit_frame[id] = -1\n",
    "\n",
    "# Y coordinate of the \"exit line\"\n",
    "y_exit = 1130\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(frame)\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            height = mean_heights[id] * 100 # cm\n",
    "            projected_point = project_point(point, height)\n",
    "\n",
    "            # Store the projected point for the aruco marker id\n",
    "            if not id in projected_points:\n",
    "                projected_points[id] = []\n",
    "                \n",
    "            projected_points[id].append(projected_point)\n",
    "\n",
    "            # Update the enter and exit frame\n",
    "            if enter_frame[id] == -1:\n",
    "                enter_frame[id] = num_frames\n",
    "\n",
    "            if y >= y_exit:\n",
    "                exit_frame[id] = num_frames\n",
    "\n",
    "            # Draw a circle at the center of the aruco marker\n",
    "            cv2.circle(frame, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for id, points in projected_points.items():\n",
    "            for projected_point in points:\n",
    "                point = projected_point.astype(int)\n",
    "                if point[1] < y_exit:\n",
    "                    cv2.circle(frame, tuple(point), 10, color_points[id], -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter frames:  {33: -1, 17: -1, 87: -1, 24: -1, 25: -1}\n",
      "Exit frames:  {33: -1, 17: -1, 87: -1, 24: -1, 25: -1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter frames: \", enter_frame)\n",
    "print(\"Exit frames: \", exit_frame)\n",
    "\n",
    "# Compute the difference between the enter and exit frame, to estimate the exit time for each id\n",
    "exit_time = {}\n",
    "basler_fps = 24\n",
    "\n",
    "for id, enter in enter_frame.items():\n",
    "    exit = exit_frame[id]\n",
    "    if enter != -1 and exit != -1:\n",
    "        exit_time[id] = (exit - enter) / basler_fps\n",
    "\n",
    "# Print exit times in seconds\n",
    "for id, time in exit_time.items():\n",
    "    print(\"ID: {} Exit time: {:.2f}s\".format(id, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synchronize the 2 streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_file_master = dataset_path + '2_depth_cameras\\master2.bag'\n",
    "pipe_master, cfg_master, profile_master, playback_master, frame_shape_master = read_bag_file(bag_file_master)\n",
    "\n",
    "bag_file_slave = dataset_path + '2_depth_cameras\\slave2.bag'\n",
    "pipe_slave, cfg_slave, profile_slave, playback_slave, frame_shape_slave = read_bag_file(bag_file_slave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master timestamp:  1720093752810.972\n",
      "Slave timestamp:  1720093761964.2336\n",
      "Timestamp difference: 9153.26171875 ms\n"
     ]
    }
   ],
   "source": [
    "frames_master = pipe_master.wait_for_frames()\n",
    "frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "depth_frame_master = frames_master.get_depth_frame()\n",
    "depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "print(\"Master timestamp: \", depth_frame_master.get_timestamp())\n",
    "print(\"Slave timestamp: \", depth_frame_slave.get_timestamp())\n",
    "print(\"Timestamp difference: {} ms\".format(timestamp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master timestamp:  1720093761946.4536\n",
      "Slave timestamp:  1720093761964.2336\n",
      "Timestamp difference: 17.780029296875 ms\n"
     ]
    }
   ],
   "source": [
    "# Let the master camera catch up with the slave camera\n",
    "while timestamp_diff > 30: # 30 ms difference\n",
    "    frames_master = pipe_master.wait_for_frames()\n",
    "    depth_frame_master = frames_master.get_depth_frame()\n",
    "    depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "\n",
    "    timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "print(\"Master timestamp: \", depth_frame_master.get_timestamp())\n",
    "print(\"Slave timestamp: \", depth_frame_slave.get_timestamp())\n",
    "print(\"Timestamp difference: {} ms\".format(timestamp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_synchronize_bag_files(file_name_master, file_name_slave):\n",
    "    # Master stream\n",
    "    pipe_master = rs.pipeline()\n",
    "    cfg_master = rs.config()\n",
    "    cfg_master.enable_device_from_file(file_name_master, repeat_playback=False)\n",
    "    profile_master = pipe_master.start(cfg_master)\n",
    "    playback_master = profile_master.get_device().as_playback()\n",
    "    playback_master.set_real_time(False) # False: no frame drop\n",
    "    \n",
    "    # Slave stream\n",
    "    pipe_slave = rs.pipeline()\n",
    "    cfg_slave = rs.config()\n",
    "    cfg_slave.enable_device_from_file(file_name_slave, repeat_playback=False)\n",
    "    profile_slave = pipe_slave.start(cfg_slave)\n",
    "    playback_slave = profile_slave.get_device().as_playback()\n",
    "    playback_slave.set_real_time(False) # False: no frame drop\n",
    "\n",
    "    # Sync the master and slave streams\n",
    "    frames_master = pipe_master.wait_for_frames()\n",
    "    frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "    depth_frame_master = frames_master.get_depth_frame()\n",
    "    depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "    depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "    depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "    timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "    while timestamp_diff > 30: # 30 ms difference\n",
    "        frames_master = pipe_master.wait_for_frames()\n",
    "        depth_frame_master = frames_master.get_depth_frame()\n",
    "        depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "\n",
    "        timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "    return pipe_master, pipe_slave, timestamp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream ended\n"
     ]
    }
   ],
   "source": [
    "pipe_master, pipe_slave, timestamp_diff = read_and_synchronize_bag_files(bag_file_master, bag_file_slave)\n",
    "wait_key = 1\n",
    "post_process = False\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames_master = pipe_master.wait_for_frames()\n",
    "        frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        depth_frame_master = frames_master.get_depth_frame()\n",
    "        depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not depth_frame_master or not depth_frame_slave:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        if post_process:\n",
    "            depth_frame_master = post_process_depth_frame(depth_frame_master)\n",
    "            depth_frame_slave = post_process_depth_frame(depth_frame_slave)\n",
    "\n",
    "        # Colorize the depth frame\n",
    "        depth_color_frame_master = colorizer.colorize(depth_frame_master)\n",
    "        depth_color_frame_slave = colorizer.colorize(depth_frame_slave)\n",
    "\n",
    "        # Convert frames to images\n",
    "        depth_color_image_master = np.asanyarray(depth_color_frame_master.get_data())\n",
    "        depth_color_image_slave = np.asanyarray(depth_color_frame_slave.get_data())\n",
    "        \n",
    "        # Stack the images horizontally\n",
    "        depth_color_image_master = cv2.resize(depth_color_image_master, (424, 240))\n",
    "        depth_color_image_slave = cv2.resize(depth_color_image_slave, (424, 240))\n",
    "        images = np.hstack((depth_color_image_master, depth_color_image_slave))\n",
    "        cv2.imshow(\"Depth Images\", images)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite('color_image.png', color_image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe_master.stop()\n",
    "    pipe_slave.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
