{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "%gui tk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_file():\n",
    "    root = tk.Tk()\n",
    "    root.withdraw()\n",
    "    file_path = filedialog.askopenfilename()\n",
    "    return file_path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Realsense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bag_file(file_name, real_time=False):\n",
    "    \"\"\"\n",
    "    Read a bag file recorded from a RealSense camera.\n",
    "\n",
    "    Parameters:\n",
    "        file_name (str): the path to the bag file.\n",
    "        real_time (bool): if True, the playback is in real time (frames may be dropped).\n",
    "    \"\"\"\n",
    "    pipe = rs.pipeline()\n",
    "    cfg = rs.config()\n",
    "    cfg.enable_device_from_file(file_name, repeat_playback=False)\n",
    "    profile = pipe.start(cfg)\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(real_time) # False: no frame drop\n",
    "    \n",
    "    # Get the frame shape\n",
    "    frames = pipe.wait_for_frames()\n",
    "    depth_frame = frames.get_depth_frame()\n",
    "    frame_shape = (depth_frame.get_height(), depth_frame.get_width())\n",
    "\n",
    "    return pipe, cfg, profile, playback, frame_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_depth_frame(depth_frame, min_distance=0, max_distance=4, decimation_magnitude = 1.0, spatial_magnitude = 2.0, spatial_smooth_alpha = 0.5, spatial_smooth_delta = 20, temporal_smooth_alpha = 0.4, temporal_smooth_delta = 20, fill_hole = False):\n",
    "    \"\"\"\n",
    "    Apply post processing filters to the depth frame of a RealSense camera.\n",
    "    More information about the filters can be found at:\n",
    "    - https://dev.intelrealsense.com/docs/post-processing-filters\n",
    "    - https://github.com/IntelRealSense/librealsense/blob/jupyter/notebooks/depth_filters.ipynb\n",
    "    - https://intelrealsense.github.io/librealsense/doxygen/namespacers2.html\n",
    "    \"\"\"\n",
    "    # Post processing possible only on the depth_frame\n",
    "    assert (depth_frame.is_depth_frame())\n",
    "\n",
    "    # Available filters\n",
    "    decimation_filter = rs.decimation_filter()\n",
    "    threshold_filter = rs.threshold_filter()\n",
    "    depth_to_disparity = rs.disparity_transform(True)\n",
    "    spatial_filter = rs.spatial_filter()\n",
    "    temporal_filter = rs.temporal_filter()\n",
    "    disparity_to_depth = rs.disparity_transform(False)\n",
    "    hole_filling = rs.hole_filling_filter(1) # https://intelrealsense.github.io/librealsense/doxygen/classrs2_1_1hole__filling__filter.html\n",
    "\n",
    "    # Apply the control parameters for the filters\n",
    "    decimation_filter.set_option(rs.option.filter_magnitude, decimation_magnitude)\n",
    "    threshold_filter.set_option(rs.option.min_distance, min_distance)\n",
    "    threshold_filter.set_option(rs.option.max_distance, max_distance)\n",
    "    spatial_filter.set_option(rs.option.filter_magnitude, spatial_magnitude)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_alpha, spatial_smooth_alpha)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_delta, spatial_smooth_delta)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_alpha, temporal_smooth_alpha)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_delta, temporal_smooth_delta)\n",
    "\n",
    "    # Apply the filters\n",
    "    # Post processing order : https://dev.intelrealsense.com/docs/post-processing-filters\n",
    "    # Depth Frame >> Decimation Filter >> Depth2Disparity Transform >> Spatial Filter >> Temporal Filter >> Disparity2Depth Transform >> Hole Filling Filter >> Filtered Depth\n",
    "    filtered_frame = decimation_filter.process(depth_frame)\n",
    "    filtered_frame = threshold_filter.process(filtered_frame)\n",
    "    filtered_frame = depth_to_disparity.process(filtered_frame)\n",
    "    filtered_frame = spatial_filter.process(filtered_frame)\n",
    "    filtered_frame = temporal_filter.process(filtered_frame)\n",
    "    filtered_frame = disparity_to_depth.process(filtered_frame)\n",
    "    if fill_hole:\n",
    "        filtered_frame = hole_filling.process(filtered_frame)\n",
    "    \n",
    "    # Cast to depth_frame so that we can use the get_distance method afterwards\n",
    "    depth_frame_filtered = filtered_frame.as_depth_frame()\n",
    "\n",
    "    return depth_frame_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "colorizer = rs.colorizer()\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "def process_frames(frames, post_process = True):\n",
    "    \"\"\"\n",
    "    Process the depth and color frames from a RealSense camera.\n",
    "    Align the depth frame to the color frame, colorize the depth frame and convert the frames to images.\n",
    "    \"\"\"\n",
    "    # Align the depth frame to the color frame\n",
    "    aligned_frames = align.process(frames)\n",
    "\n",
    "    # Get aligned frames\n",
    "    depth_frame = aligned_frames.get_depth_frame()\n",
    "    color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "    # Validate that both frames are valid\n",
    "    if not depth_frame or not color_frame:\n",
    "        return None, None, None, None\n",
    "\n",
    "    # Post process is not included in the BAG file, so we need to apply it\n",
    "    if post_process:\n",
    "        depth_frame = post_process_depth_frame(depth_frame, fill_hole=True)\n",
    "\n",
    "    # Colorize the depth frame\n",
    "    depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "    # Convert frames to images\n",
    "    depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "    color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "    return color_frame, depth_frame, color_image, depth_color_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration:  0:00:43.503743\n",
      "Frame shape:  (480, 848)\n"
     ]
    }
   ],
   "source": [
    "# Read the bag file\n",
    "bag_file = select_file()\n",
    "if not bag_file:\n",
    "    bag_file = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\bag_records\\\\2_people_height_estimation.bag\"\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "duration = playback.get_duration()\n",
    "print(\"Video duration: \", duration)\n",
    "print(\"Frame shape: \", frame_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames:  73\n"
     ]
    }
   ],
   "source": [
    "# Show the full stream\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames, True)\n",
    "\n",
    "        # Verify that the frames are valid\n",
    "        if color_frame is None or depth_frame is None:\n",
    "            continue\n",
    "        \n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        cv2.imshow(\"Depth Image\", depth_color_image)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press s to save the color image\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite('color_image.png', color_image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract height of the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the aruco detector\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_ARUCO_MIP_36h12)\n",
    "aruco_params = cv2.aruco.DetectorParameters()\n",
    "\n",
    "aruco_params.errorCorrectionRate = 0.2\n",
    "aruco_params.polygonalApproxAccuracyRate = 0.05\n",
    "aruco_params.cornerRefinementMethod = cv2.aruco.CORNER_REFINE_CONTOUR\n",
    "# aruco_params.minMarkerPerimeterRate = 0.01\n",
    "\n",
    "detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_height = 2.568 # meters\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames)\n",
    "        #color_image = cv2.cvtColor(color_image, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "        cv2.aruco.drawDetectedMarkers(color_image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Display the height with an offset to avoid overlap\n",
    "            text_position_y = 30 + k * 40\n",
    "            cv2.putText(color_image, \"ID: {} Height: {:.2f}m\".format(id, height), (10, text_position_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        cv2.imshow(\"Output\", depth_color_image)\n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream    \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{}\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(data, m=2):\n",
    "    \"\"\"\n",
    "    Remove outliers using Z-score method.\n",
    "    \"\"\"\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    filtered_data = [x for x in data if abs(x - mean) / std < m]\n",
    "    return filtered_data\n",
    "\n",
    "def calculate_mean_height(heights):\n",
    "    mean_heights = {}\n",
    "\n",
    "    for key, height_list in heights.items():\n",
    "        filtered_heights = remove_outliers(height_list)\n",
    "        if filtered_heights:\n",
    "            mean_heights[key] = np.mean(filtered_heights)\n",
    "\n",
    "    return mean_heights\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection of the point on the floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching points on the floor, 130cm and 170cm\n",
    "points_floor = np.array([\n",
    "    [1042, 737, 0],\n",
    "    [1161, 747, 0],\n",
    "    [1287, 760, 0],\n",
    "    [1405, 771, 0],\n",
    "    [1025, 861, 0],\n",
    "    [1148, 870, 0],\n",
    "    [1274, 882, 0],\n",
    "    [1397, 891, 0],\n",
    "    [1011, 986, 0],\n",
    "    [1138, 997, 0],\n",
    "    [1265, 1007, 0],\n",
    "    [1392, 1018, 0],\n",
    "    [996, 1111, 0],\n",
    "    [1128, 1120, 0],\n",
    "    [1257, 1128, 0],\n",
    "    [1382, 1134, 0]\n",
    "])\n",
    "\n",
    "points_130 = np.array([\n",
    "    [805, 395, 130],\n",
    "    [1060, 420, 130],\n",
    "    [1320, 449, 130],\n",
    "    [1554, 473, 130],\n",
    "    [777, 651, 130],\n",
    "    [1040, 677, 130],\n",
    "    [1301, 702, 130],\n",
    "    [1549, 731, 130],\n",
    "    [746, 914, 130],\n",
    "    [1024, 937, 130],\n",
    "    [1286, 960, 130],\n",
    "    [1532, 976, 130],\n",
    "    [722, 1130, 130],\n",
    "    [1017, 1178, 130],\n",
    "    [1257, 1179, 130],\n",
    "    [1507, 1208, 130]\n",
    "])\n",
    "\n",
    "points_170 = np.array([\n",
    "    [598, 91, 170],\n",
    "    [970, 118, 170],\n",
    "    [1346, 167, 170],\n",
    "    [1693, 206, 170],\n",
    "    [537, 445, 170],\n",
    "    [931, 489, 170],\n",
    "    [1323, 536, 170],\n",
    "    [1685, 576, 170],\n",
    "    [481, 842, 170],\n",
    "    [906, 873, 170],\n",
    "    [1301, 916, 170],\n",
    "    [1664, 933, 170],\n",
    "    [444, 1172, 170],\n",
    "    [895, 1240, 170],\n",
    "    [1261, 1241, 170],\n",
    "    [1634, 1278, 170]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_file = \"C:\\\\Users\\\\Robin\\\\Documents\\Stage2024\\\\Dataset\\\\points_measurements_projection\\\\mini_experiment\\\\Image__2024-03-20__16-55-35.png\"\n",
    "image = cv2.imread(points_floor_file)\n",
    "\n",
    "for pf, p130, p170 in zip(points_floor, points_130, points_170):\n",
    "    cv2.line(image, pf[:2], p130[:2], (255, 0, 0), 1)\n",
    "    cv2.line(image, p130[:2], p170[:2], (0, 255, 0), 1)\n",
    "\n",
    "cv2.namedWindow('Projection Lines', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Lines', image)\n",
    "cv2.resizeWindow('Projection Lines', 864, 648)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 3 homographies\n",
    "points_floor_2d = points_floor[:, :2]\n",
    "points_130_2d = points_130[:, :2]\n",
    "points_170_2d = points_170[:, :2]\n",
    "\n",
    "H_0, _ = cv2.findHomography(points_floor_2d, points_floor_2d) # Identity matrix (hopefully :))\n",
    "H_130, _ = cv2.findHomography(points_130_2d, points_floor_2d)\n",
    "H_170, _ = cv2.findHomography(points_170_2d, points_floor_2d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_homography1(z, H_0, H_130, H_170):\n",
    "    \"\"\"\n",
    "    Interpolate the homography matrix for a given height, between 0 and 210 cm.\n",
    "    \"\"\"\n",
    "    if z <= 130: # linear interpolation between 0 and 130\n",
    "        alpha = z / 130 # /40 to normalize the alpha value (0 <= z <= 130)\n",
    "        return (1 - alpha) * H_0 + alpha * H_130\n",
    "    \n",
    "    elif z <= 170: # linear interpolation between 130 and 170\n",
    "        alpha = (z - 130) / 40 # /40 to normalize the alpha value (130 < z <= 170)\n",
    "        return (1 - alpha) * H_130 + alpha * H_170\n",
    "    \n",
    "    else: # extrapolation between 170 and 210\n",
    "        # Basically, from 170 cm, the transformation continues to change in the same way as it did between 130 cm and 170 cm\n",
    "        H_210 =  H_170 + (H_170 - H_130)\n",
    "        alpha = (z - 170) / 40 # /40 to normalize the alpha value (170 < z <= 210)\n",
    "        return (1 - alpha) * H_170 + alpha * H_210\n",
    "\n",
    "def project_point(point, z, H):\n",
    "    \"\"\"\n",
    "    Project a 3D point on the floor, given its 3D coordinates and the homography matrix.\n",
    "\n",
    "    Parameters:\n",
    "        point (np.array): the 2D point coordinates [x, y].\n",
    "        z (float): the height of the point.\n",
    "    \"\"\"\n",
    "    point_homogeneous = np.hstack([point, 1])\n",
    "    projected_point_homogeneous = H @ point_homogeneous\n",
    "    projected_point = projected_point_homogeneous[:2] / projected_point_homogeneous[2]\n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example to test the homography interpolation\n",
    "image = cv2.imread(points_floor_file)\n",
    "\n",
    "cv2.namedWindow('Projection Points', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Points', image)\n",
    "cv2.resizeWindow('Projection Points', 864, 648)\n",
    "\n",
    "# Draw the points at 170 cm\n",
    "for point in points_170.astype(int):\n",
    "    cv2.circle(image, tuple(point[:2]), 10, (255, 0, 0), -1)\n",
    "\n",
    "def on_trackbar(z):\n",
    "    z = int(z)\n",
    "    H = interpolate_homography1(z, H_0, H_130, H_170)\n",
    "\n",
    "    projected_points = []\n",
    "\n",
    "    for point in points_170:\n",
    "        point = point[:2]\n",
    "        projected_point = project_point(point, z, H)\n",
    "        projected_points.append(projected_point)\n",
    "\n",
    "    image_copy = image.copy()\n",
    "    for point in projected_points:\n",
    "        point = point.astype(int)\n",
    "        cv2.circle(image_copy, tuple(point), 10, (0, 0, 255), -1)\n",
    "        \n",
    "    cv2.imshow('Projection Points', image_copy)\n",
    "\n",
    "cv2.createTrackbar('Height', 'Projection Points', 170, 210, on_trackbar)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "baslar_file = select_file()\n",
    "if not baslar_file:\n",
    "    baslar_file = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\evacuation_experiments\\\\solo_experiment\\\\rec.mp4\"\n",
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "height = 180 # cm\n",
    "projected_points = []\n",
    "aruco_ids = set()\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, image = cap.read()\n",
    "\n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(image)\n",
    "        cv2.aruco.drawDetectedMarkers(image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            aruco_ids.add(id)\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            H = interpolate_homography1(height, H_0, H_130, H_170)\n",
    "            projected_point = project_point(point, height, H)\n",
    "            projected_points.append(projected_point)\n",
    "\n",
    "            cv2.circle(image, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for projected_point in projected_points:\n",
    "            point = projected_point.astype(int)\n",
    "            cv2.circle(image, tuple(point), 10, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', image)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate exit times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimate heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' camera_height = 2.568 # meters, ground truth\\n\\npipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\\nnum_frames = 0\\n\\n# Data structure to store the a list of height for every aruco marker id\\nheights = {}\\n\\ntry:\\n    while True:\\n        # Get frameset of color and depth\\n        frames = pipe.wait_for_frames()\\n\\n        # Process the frames\\n        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames)\\n\\n        # Detect the aruco markers\\n        corners, ids, rejected = detector.detectMarkers(color_image)\\n\\n        for k in range(len(corners)):\\n            id = ids[k][0]\\n            c = corners[k][0]\\n\\n            # Calculate the distance using the center of the aruco marker\\n            x = int(c[:, 0].sum() / 4)\\n            y = int(c[:, 1].sum() / 4)\\n            distance = depth_frame.get_distance(x, y)\\n\\n            # Calculate the height\\n            height = camera_height - distance\\n\\n            # Store the height for the aruco marker id\\n            if not id in heights:\\n                heights[id] = []\\n                \\n            heights[id].append(height)\\n\\n        num_frames += 1\\n           \\n# Catch exception if the stream is ended\\nexcept RuntimeError:\\n    print(\"Stream ended\")\\n        \\nfinally:\\n    # Stop streaming\\n    pipe.stop()\\n\\nprint(\"Total number of frames: \", num_frames)\\n\\n# Remove outliers and calculate the mean height for each aruco marker id\\nmean_heights = calculate_mean_height(heights)\\nprint(mean_heights) '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" camera_height = 2.568 # meters, ground truth\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        color_frame, depth_frame, color_image, depth_color_image = process_frames(frames)\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights) \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{33: 1.75, 17: 1.75, 87: 1.75, 24: 1.75, 25: 1.75}\n"
     ]
    }
   ],
   "source": [
    "# Hardcoded heights because the realsense camera was not properly placed\n",
    "aruco_ids = {33, 17, 87, 24, 25}\n",
    "mean_heights = {key: 1.75 for key in aruco_ids}\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "baslar_file = select_file()\n",
    "if not baslar_file:\n",
    "    baslar_file = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\evacuation_experiments\\\\group_experiment\\\\first.mp4\"\n",
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "num_frames = 0\n",
    "\n",
    "# Dictionary to store the projected points for each aruco marker id, and the color for each id, only for visualization\n",
    "projected_points = {}\n",
    "color_points = {}\n",
    "for id, _ in mean_heights.items():\n",
    "    color_points[id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "# Data structure to store the enter and exit frame for each aruco marker id\n",
    "enter_frame = {}\n",
    "exit_frame = {}\n",
    "for id, _ in mean_heights.items():\n",
    "    enter_frame[id] = -1\n",
    "    exit_frame[id] = -1\n",
    "\n",
    "# Y coordinate of the \"exit line\"\n",
    "y_exit = 1130\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(frame)\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            height = mean_heights[id] * 100 # cm\n",
    "            H = interpolate_homography1(height, H_0, H_130, H_170)\n",
    "            projected_point = project_point(point, height, H)\n",
    "\n",
    "            # Store the projected point for the aruco marker id\n",
    "            if not id in projected_points:\n",
    "                projected_points[id] = []\n",
    "                \n",
    "            projected_points[id].append(projected_point)\n",
    "\n",
    "            # Update the enter and exit frame\n",
    "            if enter_frame[id] == -1:\n",
    "                enter_frame[id] = num_frames\n",
    "\n",
    "            if y >= y_exit:\n",
    "                exit_frame[id] = num_frames\n",
    "\n",
    "            # Draw a circle at the center of the aruco marker\n",
    "            cv2.circle(frame, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for id, points in projected_points.items():\n",
    "            for projected_point in points:\n",
    "                point = projected_point.astype(int)\n",
    "                if point[1] < y_exit:\n",
    "                    cv2.circle(frame, tuple(point), 10, color_points[id], -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter frames:  {33: -1, 17: -1, 87: -1, 24: -1, 25: -1}\n",
      "Exit frames:  {33: -1, 17: -1, 87: -1, 24: -1, 25: -1}\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter frames: \", enter_frame)\n",
    "print(\"Exit frames: \", exit_frame)\n",
    "\n",
    "# Compute the difference between the enter and exit frame, to estimate the exit time for each id\n",
    "exit_time = {}\n",
    "basler_fps = 24\n",
    "\n",
    "for id, enter in enter_frame.items():\n",
    "    exit = exit_frame[id]\n",
    "    if enter != -1 and exit != -1:\n",
    "        exit_time[id] = (exit - enter) / basler_fps\n",
    "\n",
    "# Print exit times in seconds\n",
    "for id, time in exit_time.items():\n",
    "    print(\"ID: {} Exit time: {:.2f}s\".format(id, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial wall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Synchronize the 2 streams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "bag_file_master = select_file()\n",
    "if not bag_file_master:\n",
    "    bag_file_master = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\bag_records\\\\realsense_sync\\\\master2.bag\"\n",
    "pipe_master, cfg_master, profile_master, playback_master, frame_shape_master = read_bag_file(bag_file_master)\n",
    "\n",
    "bag_file_slave = select_file()\n",
    "if not bag_file_slave:\n",
    "    bag_file_slave = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\bag_records\\\\realsense_sync\\\\slave2.bag\"\n",
    "pipe_slave, cfg_slave, profile_slave, playback_slave, frame_shape_slave = read_bag_file(bag_file_slave)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master timestamp:  1720093892611.5762\n",
      "Slave timestamp:  1720093896731.1372\n",
      "Timestamp difference: 4119.56103515625 ms\n"
     ]
    }
   ],
   "source": [
    "frames_master = pipe_master.wait_for_frames()\n",
    "frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "depth_frame_master = frames_master.get_depth_frame()\n",
    "depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "print(\"Master timestamp: \", depth_frame_master.get_timestamp())\n",
    "print(\"Slave timestamp: \", depth_frame_slave.get_timestamp())\n",
    "print(\"Timestamp difference: {} ms\".format(timestamp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Master timestamp:  1720093896712.5283\n",
      "Slave timestamp:  1720093896731.1372\n",
      "Timestamp difference: 18.60888671875 ms\n"
     ]
    }
   ],
   "source": [
    "# Let the master camera catch up with the slave camera\n",
    "while timestamp_diff > 30: # 30 ms difference\n",
    "    frames_master = pipe_master.wait_for_frames()\n",
    "    depth_frame_master = frames_master.get_depth_frame()\n",
    "    depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "\n",
    "    timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "print(\"Master timestamp: \", depth_frame_master.get_timestamp())\n",
    "print(\"Slave timestamp: \", depth_frame_slave.get_timestamp())\n",
    "print(\"Timestamp difference: {} ms\".format(timestamp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_and_synchronize_bag_files(file_name_master, file_name_slave):\n",
    "    \"\"\"\n",
    "    Read and synchronize two bag files recorded from two RealSense cameras.\n",
    "    Synchronize the master and slave streams based on the global timestamps.\n",
    "    \"\"\"\n",
    "    # Master stream\n",
    "    pipe_master = rs.pipeline()\n",
    "    cfg_master = rs.config()\n",
    "    cfg_master.enable_device_from_file(file_name_master, repeat_playback=False)\n",
    "    profile_master = pipe_master.start(cfg_master)\n",
    "    playback_master = profile_master.get_device().as_playback()\n",
    "    playback_master.set_real_time(False) # False: no frame drop\n",
    "    \n",
    "    # Slave stream\n",
    "    pipe_slave = rs.pipeline()\n",
    "    cfg_slave = rs.config()\n",
    "    cfg_slave.enable_device_from_file(file_name_slave, repeat_playback=False)\n",
    "    profile_slave = pipe_slave.start(cfg_slave)\n",
    "    playback_slave = profile_slave.get_device().as_playback()\n",
    "    playback_slave.set_real_time(False) # False: no frame drop\n",
    "\n",
    "    # Sync the master and slave streams\n",
    "    frames_master = pipe_master.wait_for_frames()\n",
    "    frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "    depth_frame_master = frames_master.get_depth_frame()\n",
    "    depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "    depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "    depth_timestamp_slave = depth_frame_slave.get_timestamp()\n",
    "    timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "    while timestamp_diff > 30: # 30 ms difference\n",
    "        frames_master = pipe_master.wait_for_frames()\n",
    "        depth_frame_master = frames_master.get_depth_frame()\n",
    "        depth_timestamp_master = depth_frame_master.get_timestamp()\n",
    "\n",
    "        timestamp_diff = abs(depth_timestamp_master - depth_timestamp_slave)\n",
    "\n",
    "    return pipe_master, pipe_slave, timestamp_diff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipe_master, pipe_slave, timestamp_diff = read_and_synchronize_bag_files(bag_file_master, bag_file_slave)\n",
    "wait_key = 1\n",
    "post_process = False\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames_master = pipe_master.wait_for_frames()\n",
    "        frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        depth_frame_master = frames_master.get_depth_frame()\n",
    "        depth_frame_slave = frames_slave.get_depth_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not depth_frame_master or not depth_frame_slave:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        if post_process:\n",
    "            depth_frame_master = post_process_depth_frame(depth_frame_master)\n",
    "            depth_frame_slave = post_process_depth_frame(depth_frame_slave)\n",
    "\n",
    "        # Colorize the depth frame\n",
    "        depth_color_frame_master = colorizer.colorize(depth_frame_master)\n",
    "        depth_color_frame_slave = colorizer.colorize(depth_frame_slave)\n",
    "\n",
    "        # Convert frames to images\n",
    "        depth_color_image_master = np.asanyarray(depth_color_frame_master.get_data())\n",
    "        depth_color_image_slave = np.asanyarray(depth_color_frame_slave.get_data())\n",
    "        \n",
    "        # Stack the images horizontally\n",
    "        frame_shape = (depth_frame_master.get_height(), depth_frame_master.get_width())\n",
    "        new_frame_shape = (int(frame_shape[1] / 1.5), int(frame_shape[0] / 1.5))\n",
    "        depth_color_image_master = cv2.resize(depth_color_image_master, new_frame_shape)\n",
    "        depth_color_image_slave = cv2.resize(depth_color_image_slave, new_frame_shape)\n",
    "        images = np.hstack((depth_color_image_master, depth_color_image_slave))\n",
    "        cv2.imshow(\"Depth Images\", images)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe_master.stop()\n",
    "    pipe_slave.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Ground projection comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Homography for the first camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_cam1 = np.array([\n",
    "    [267, 140], [369, 133], [473, 132], \n",
    "    [267, 218], [374, 210], [472, 206], \n",
    "    [269, 301], [377, 298], [479, 302]\n",
    "])\n",
    "\n",
    "points_100_cam1 = np.array([\n",
    "    [167, 71], [333, 60], [510, 57], \n",
    "    [169, 200], [346, 188], [510, 182],\n",
    "    [168, 340], [351, 334], [529, 340]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_file1 = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\points_measurements_projection\\\\artficial_experiment\\\\points_ground_1\\\\0.png\"\n",
    "image = cv2.imread(points_floor_file1)\n",
    "\n",
    "for pf, p100 in zip(points_floor_cam1, points_100_cam1):\n",
    "    cv2.line(image, pf[:2], p100[:2], (255, 0, 0), 1)\n",
    "\n",
    "cv2.namedWindow('Projection Lines', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Lines', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 2 homographies\n",
    "H_0_cam1, _ = cv2.findHomography(points_floor_cam1, points_floor_cam1) # Identity matrix (hopefully :))\n",
    "H_100_cam1, _ = cv2.findHomography(points_100_cam1, points_floor_cam1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interpolate_homography2(z, H_0, H_100):\n",
    "    \"\"\"\n",
    "    Interpolate the homography matrix for a given height, between 0 and 200 cm.\n",
    "    \"\"\"\n",
    "    if z <= 100: # linear interpolation between 0 and 100\n",
    "        alpha = z / 100\n",
    "        return (1 - alpha) * H_0 + alpha * H_100\n",
    "    \n",
    "    else: # extrapolation between 100 and 200\n",
    "        H_200 =  H_100 + (H_100 - H_0)\n",
    "        alpha = (z - 100) / 100\n",
    "        return (1 - alpha) * H_100 + alpha * H_200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example to test the homography interpolation\n",
    "image = cv2.imread(points_floor_file1)\n",
    "\n",
    "# Draw the points at 100 cm\n",
    "for point in points_100_cam1.astype(int):\n",
    "    cv2.circle(image, tuple(point), 3, (255, 0, 0), -1)\n",
    "\n",
    "cv2.namedWindow('Projection Points', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Points', image)\n",
    "\n",
    "def on_trackbar(z):\n",
    "    z = int(z)\n",
    "    H = interpolate_homography2(z, H_0_cam1, H_100_cam1)\n",
    "\n",
    "    projected_points = []\n",
    "\n",
    "    for point in points_100_cam1:\n",
    "        projected_point = project_point(point, z, H)\n",
    "        projected_points.append(projected_point)\n",
    "\n",
    "    image_copy = image.copy()\n",
    "    for point in projected_points:\n",
    "        point = point.astype(int)\n",
    "        cv2.circle(image_copy, tuple(point), 3, (0, 0, 255), -1)\n",
    "\n",
    "    cv2.imshow('Projection Points', image_copy)\n",
    "\n",
    "cv2.createTrackbar('Height', 'Projection Points', 100, 200, on_trackbar)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute Homography for the second camera"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_cam2 = np.array([\n",
    "    [473, 164], [471, 240], [473, 333], \n",
    "    [373, 329], [370, 241], [365, 159], \n",
    "    [263, 165], [263, 245], [263, 329]\n",
    "])\n",
    "\n",
    "points_100_cam2 = np.array([\n",
    "    [513, 121], [508, 249], [519, 406], \n",
    "    [345, 395], [342, 251], [331, 120], \n",
    "    [159, 125], [161, 259], [162, 399]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "points_floor_file2 = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\points_measurements_projection\\\\artficial_experiment\\\\points_ground_2\\\\0.png\"\n",
    "image = cv2.imread(points_floor_file2)\n",
    "\n",
    "for pf, p100 in zip(points_floor_cam2, points_100_cam2):\n",
    "    cv2.line(image, pf[:2], p100[:2], (255, 0, 0), 1)\n",
    "\n",
    "cv2.namedWindow('Projection Lines', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Lines', image)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 2 homographies\n",
    "H_0_cam2, _ = cv2.findHomography(points_floor_cam2, points_floor_cam2) # Identity matrix (hopefully :))\n",
    "H_100_cam2, _ = cv2.findHomography(points_100_cam2, points_floor_cam2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example to test the homography interpolation\n",
    "image = cv2.imread(points_floor_file2)\n",
    "\n",
    "# Draw the points at 100 cm\n",
    "for point in points_100_cam2.astype(int):\n",
    "    cv2.circle(image, tuple(point), 3, (255, 0, 0), -1)\n",
    "\n",
    "cv2.namedWindow('Projection Points', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Points', image)\n",
    "\n",
    "def on_trackbar(z):\n",
    "    z = int(z)\n",
    "    H = interpolate_homography2(z, H_0_cam2, H_100_cam2)\n",
    "\n",
    "    projected_points = []\n",
    "\n",
    "    for point in points_100_cam2:\n",
    "        projected_point = project_point(point, z, H)\n",
    "        projected_points.append(projected_point)\n",
    "\n",
    "    image_copy = image.copy()\n",
    "    for point in projected_points:\n",
    "        point = point.astype(int)\n",
    "        cv2.circle(image_copy, tuple(point), 3, (0, 0, 255), -1)\n",
    "\n",
    "    cv2.imshow('Projection Points', image_copy)\n",
    "\n",
    "cv2.createTrackbar('Height', 'Projection Points', 100, 200, on_trackbar)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare ground projections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp difference: 28.009033203125 ms\n"
     ]
    }
   ],
   "source": [
    "bag_file_master = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\bag_records\\\\artificial_exit_line\\\\robin_master.bag\"\n",
    "\n",
    "bag_file_slave = \"C:\\\\Users\\\\Robin\\\\Documents\\\\Stage2024\\\\Dataset\\\\bag_records\\\\artificial_exit_line\\\\robin_slave.bag\"\n",
    "\n",
    "pipe_master, pipe_slave, timestamp_diff = read_and_synchronize_bag_files(bag_file_master, bag_file_slave)\n",
    "print(\"Timestamp difference: {} ms\".format(timestamp_diff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frames(frames_master, frames_slave):\n",
    "    # Process the frames\n",
    "    infrared_frame_master = frames_master.get_infrared_frame()\n",
    "    infrared_image_master = np.asanyarray(infrared_frame_master.get_data())\n",
    "    image_master = cv2.cvtColor(infrared_image_master, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    infrared_frame_slave = frames_slave.get_infrared_frame()\n",
    "    infrared_image_slave = np.asanyarray(infrared_frame_slave.get_data())\n",
    "    image_slave = cv2.cvtColor(infrared_image_slave, cv2.COLOR_GRAY2BGR)\n",
    "\n",
    "    return image_master, image_slave\n",
    "\n",
    "def detect_markers_and_project(image, detector, H0, H100, height, projected_points):\n",
    "    # Detect the aruco markers\n",
    "    corners, ids, rejected = detector.detectMarkers(image)\n",
    "    cv2.aruco.drawDetectedMarkers(image, corners, ids)\n",
    "\n",
    "    for k in range(len(corners)):\n",
    "        id = ids[k][0]\n",
    "        c = corners[k][0]\n",
    "\n",
    "        # Center of the aruco marker\n",
    "        x = int(c[:, 0].sum() / 4)\n",
    "        y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "        # Project the point on the floor\n",
    "        point = (x, y)\n",
    "        H = interpolate_homography2(height, H0, H100)\n",
    "        projected_point = project_point(point, height, H)\n",
    "        projected_points.append(projected_point)\n",
    "\n",
    "        cv2.circle(image, point, 4, (0, 0, 255), -1)\n",
    "\n",
    "    return image, projected_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "wait_key = 1\n",
    "\n",
    "height = 180\n",
    "projected_points_cam1 = []\n",
    "projected_points_cam2 = []\n",
    "\n",
    "def clear_points(val):\n",
    "    if val == 1:\n",
    "        projected_points_cam1.clear()\n",
    "        projected_points_cam2.clear()\n",
    "\n",
    "def get_pos(event, x, y, flags, param):\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        print(\"(x : {}, y : {})\".format(x, y))\n",
    "\n",
    "cv2.namedWindow('Frame')\n",
    "cv2.setMouseCallback('Frame', get_pos)\n",
    "cv2.createTrackbar('Clear', 'Frame', 0, 1, clear_points)\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames_master = pipe_master.wait_for_frames()\n",
    "        frames_slave = pipe_slave.wait_for_frames()\n",
    "\n",
    "        # Process the frames\n",
    "        image_master, image_slave = process_frames(frames_master, frames_slave)\n",
    "\n",
    "        # Detect the aruco markers and project the points\n",
    "        image_master, projected_points_cam1 = detect_markers_and_project(image_master, detector, H_0_cam2, H_100_cam2, height, projected_points_cam1)\n",
    "        image_slave, projected_points_cam2 = detect_markers_and_project(image_slave, detector, H_0_cam1, H_100_cam1, height, projected_points_cam2)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for projected_point in projected_points_cam1:\n",
    "            point = projected_point.astype(int)\n",
    "            cv2.circle(image_master, tuple(point), 4, (0, 255, 255), -1)\n",
    "\n",
    "        for projected_point in projected_points_cam2:\n",
    "            point = projected_point.astype(int)\n",
    "            cv2.circle(image_slave, tuple(point), 4, (0, 255, 255), -1)\n",
    "\n",
    "        # resize images with ratio 0.75\n",
    "        image_master = cv2.resize(image_master, (0, 0), fx=0.75, fy=0.75)\n",
    "        y_exit_master = 213\n",
    "        cv2.line(image_master, (0, y_exit_master), (image_master.shape[1], y_exit_master), (0, 0, 255), 1)\n",
    "\n",
    "        image_slave = cv2.resize(image_slave, (0, 0), fx=0.75, fy=0.75)\n",
    "        y_exit_slave = 189\n",
    "        cv2.line(image_slave, (0, y_exit_slave), (image_slave.shape[1], y_exit_slave), (0, 0, 255), 1)\n",
    "\n",
    "        images = np.hstack((image_master, image_slave))\n",
    "        cv2.imshow('Frame', images)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            wait_key = 1 if wait_key == 0 else 0\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "\n",
    "# Release the stream    \n",
    "finally:\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe_master.stop()\n",
    "    pipe_slave.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
