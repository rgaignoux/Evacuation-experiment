{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = '..\\\\..\\\\Dataset'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup (Realsense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bag_file(file_name, real_time=False):\n",
    "    pipe = rs.pipeline()\n",
    "    cfg = rs.config()\n",
    "    cfg.enable_device_from_file(file_name, repeat_playback=False)\n",
    "    profile = pipe.start(cfg)\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(real_time) # False: no frame drop\n",
    "    \n",
    "    # Get the frame shape of the color sensor\n",
    "    frames = pipe.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    frame_shape = (color_frame.get_height(), color_frame.get_width())\n",
    "\n",
    "    return pipe, cfg, profile, playback, frame_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration:  0:00:43.503743\n",
      "Frame shape:  (480, 848)\n"
     ]
    }
   ],
   "source": [
    "# Read the bag file\n",
    "bag_file = dataset_path + \"\\\\me_and_sasa.bag\"\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "duration = playback.get_duration()\n",
    "print(\"Video duration: \", duration)\n",
    "print(\"Frame shape: \", frame_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_depth_frame(depth_frame, min_distance=0, max_distance=1.5, decimation_magnitude = 1.0, spatial_magnitude = 2.0, spatial_smooth_alpha = 0.5, spatial_smooth_delta = 20, temporal_smooth_alpha = 0.4, temporal_smooth_delta = 20):\n",
    "    # Post processing possible only on the depth_frame\n",
    "    assert (depth_frame.is_depth_frame())\n",
    "\n",
    "    # Available filters\n",
    "    decimation_filter = rs.decimation_filter()\n",
    "    threshold_filter = rs.threshold_filter()\n",
    "    depth_to_disparity = rs.disparity_transform(True)\n",
    "    spatial_filter = rs.spatial_filter()\n",
    "    temporal_filter = rs.temporal_filter()\n",
    "    disparity_to_depth = rs.disparity_transform(False)\n",
    "    hole_filling = rs.hole_filling_filter(1) # https://intelrealsense.github.io/librealsense/doxygen/classrs2_1_1hole__filling__filter.html\n",
    "\n",
    "    # Apply the control parameters for the filters\n",
    "    decimation_filter.set_option(rs.option.filter_magnitude, decimation_magnitude)\n",
    "    threshold_filter.set_option(rs.option.min_distance, min_distance)\n",
    "    threshold_filter.set_option(rs.option.max_distance, max_distance)\n",
    "    spatial_filter.set_option(rs.option.filter_magnitude, spatial_magnitude)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_alpha, spatial_smooth_alpha)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_delta, spatial_smooth_delta)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_alpha, temporal_smooth_alpha)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_delta, temporal_smooth_delta)\n",
    "\n",
    "    # Apply the filters\n",
    "    # Post processing order : https://dev.intelrealsense.com/docs/post-processing-filters\n",
    "    # Depth Frame >> Decimation Filter >> Depth2Disparity Transform >> Spatial Filter >> Temporal Filter >> Disparity2Depth Transform >> Hole Filling Filter >> Filtered Depth\n",
    "    filtered_frame = decimation_filter.process(depth_frame)\n",
    "    filtered_frame = threshold_filter.process(filtered_frame)\n",
    "    filtered_frame = depth_to_disparity.process(filtered_frame)\n",
    "    filtered_frame = spatial_filter.process(filtered_frame)\n",
    "    filtered_frame = temporal_filter.process(filtered_frame)\n",
    "    filtered_frame = disparity_to_depth.process(filtered_frame)\n",
    "    filtered_frame = hole_filling.process(filtered_frame)\n",
    "    \n",
    "    # Cast to depth_frame so that we can use the get_distance method afterwards\n",
    "    depth_frame_filtered = filtered_frame.as_depth_frame()\n",
    "\n",
    "    return depth_frame_filtered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames:  268\n"
     ]
    }
   ],
   "source": [
    "colorizer = rs.colorizer()\n",
    "\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "# Read the full stream\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        depth_frame = post_process_depth_frame(aligned_depth_frame)\n",
    "\n",
    "        # Colorize the depth frame\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert frames to images\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        cv2.imshow(\"Depth Image\", depth_color_image)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        \"\"\" # Press s to write the images on disk\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite(\".\\\\images\\\\color_images\\\\images\\\\color_\" + str(num_frames) + \".png\", color_image)\n",
    "            cv2.imwrite(\".\\\\images\\\\depth_images\\\\depth_\" + str(num_frames) + \".png\", depth_color_image) \"\"\"\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract height of the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a mask to keep ony the ROI, under the camera\n",
    "# mask = np.zeros(frame_shape, dtype=np.uint8)\n",
    "# center_y, center_x = frame_shape[0] // 2, frame_shape[1] // 2\n",
    "\n",
    "# # Mask size\n",
    "# half_height = frame_shape[0] // 3\n",
    "# top_left = (0, half_height)\n",
    "# bottom_right = (848, 480)\n",
    "\n",
    "# cv2.rectangle(mask, top_left, bottom_right, 255, thickness=-1)\n",
    "\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the aruco detector\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_ARUCO_MIP_36h12)\n",
    "aruco_params = cv2.aruco.DetectorParameters()\n",
    "\n",
    "aruco_params.errorCorrectionRate = 0.2 # default 0.6\n",
    "aruco_params.polygonalApproxAccuracyRate = 0.05 # default 0.03\n",
    "\n",
    "detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_height = 2.568 # meters, ground truth\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        depth_frame = post_process_depth_frame(aligned_depth_frame)\n",
    "\n",
    "        # Colorize the depth frame\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        # Convert frames to images\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "        output_image = depth_color_image.copy()\n",
    "        cv2.aruco.drawDetectedMarkers(output_image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Display the height with an offset to avoid overlap\n",
    "            text_position_y = 30 + k * 40\n",
    "            cv2.putText(output_image, \"ID: {} Height: {:.2f}m\".format(id, height), (10, text_position_y), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        cv2.imshow(\"Output\", output_image)\n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{88: 1.727318140961907, 24: 1.824235255914576}\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(data, m=2):\n",
    "    # Z-score method\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    filtered_data = [x for x in data if abs(x - mean) / std < m]\n",
    "    return filtered_data\n",
    "\n",
    "def calculate_mean_height(heights):\n",
    "    mean_heights = {}\n",
    "\n",
    "    for key, height_list in heights.items():\n",
    "        filtered_heights = remove_outliers(height_list)\n",
    "        if filtered_heights:\n",
    "            mean_heights[key] = np.mean(filtered_heights)\n",
    "\n",
    "    return mean_heights\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projection of the point on the floor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matching points on the floor, 130cm and 170cm\n",
    "points_floor = np.array([\n",
    "    [1042, 737, 0],\n",
    "    [1161, 747, 0],\n",
    "    [1287, 760, 0],\n",
    "    [1405, 771, 0],\n",
    "    [1025, 861, 0],\n",
    "    [1148, 870, 0],\n",
    "    [1274, 882, 0],\n",
    "    [1397, 891, 0],\n",
    "    [1011, 986, 0],\n",
    "    [1138, 997, 0],\n",
    "    [1265, 1007, 0],\n",
    "    [1392, 1018, 0],\n",
    "    [996, 1111, 0],\n",
    "    [1128, 1120, 0],\n",
    "    [1257, 1128, 0],\n",
    "    [1382, 1134, 0]\n",
    "])\n",
    "\n",
    "points_130 = np.array([\n",
    "    [805, 395, 130],\n",
    "    [1060, 420, 130],\n",
    "    [1320, 449, 130],\n",
    "    [1554, 473, 130],\n",
    "    [777, 651, 130],\n",
    "    [1040, 677, 130],\n",
    "    [1301, 702, 130],\n",
    "    [1549, 731, 130],\n",
    "    [746, 914, 130],\n",
    "    [1024, 937, 130],\n",
    "    [1286, 960, 130],\n",
    "    [1532, 976, 130],\n",
    "    [722, 1130, 130],\n",
    "    [1017, 1178, 130],\n",
    "    [1257, 1179, 130],\n",
    "    [1507, 1208, 130]\n",
    "])\n",
    "\n",
    "points_170 = np.array([\n",
    "    [598, 91, 170],\n",
    "    [970, 118, 170],\n",
    "    [1346, 167, 170],\n",
    "    [1693, 206, 170],\n",
    "    [537, 445, 170],\n",
    "    [931, 489, 170],\n",
    "    [1323, 536, 170],\n",
    "    [1685, 576, 170],\n",
    "    [481, 842, 170],\n",
    "    [906, 873, 170],\n",
    "    [1301, 916, 170],\n",
    "    [1664, 933, 170],\n",
    "    [444, 1172, 170],\n",
    "    [895, 1240, 170],\n",
    "    [1261, 1241, 170],\n",
    "    [1634, 1278, 170]\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_path = dataset_path + '\\\\SINGLE\\\\Image__2024-03-20__16-55-35.png'\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "for pf, p130, p170 in zip(points_floor, points_130, points_170):\n",
    "    cv2.line(image, pf[:2], p130[:2], (255, 0, 0), 1)\n",
    "    cv2.line(image, p130[:2], p170[:2], (0, 255, 0), 1)\n",
    "\n",
    "cv2.namedWindow('Projection Lines', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Lines', image)\n",
    "cv2.resizeWindow('Projection Lines', 864, 648)\n",
    "cv2.waitKey()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the 3 homographies\n",
    "points_floor_2d = points_floor[:, :2]\n",
    "points_130_2d = points_130[:, :2]\n",
    "points_170_2d = points_170[:, :2]\n",
    "\n",
    "H_0, _ = cv2.findHomography(points_floor_2d, points_floor_2d) # Identity matrix (hopefully :))\n",
    "H_130, _ = cv2.findHomography(points_130_2d, points_floor_2d)\n",
    "H_170, _ = cv2.findHomography(points_170_2d, points_floor_2d)\n",
    "\n",
    "# Interpolate the homography for a given height, between 0 and 210 cm\n",
    "def interpolate_homography(z, H_0, H_130, H_170):\n",
    "    if z <= 130: # linear interpolation between 0 and 130\n",
    "        alpha = z / 130\n",
    "        return (1 - alpha) * H_0 + alpha * H_130\n",
    "    \n",
    "    elif z <= 170: # linear interpolation between 130 and 170\n",
    "        alpha = (z - 130) / 40 # /40 to normalize the alpha value (130 < z <= 170)\n",
    "        return (1 - alpha) * H_130 + alpha * H_170\n",
    "    \n",
    "    else: # extrapolation between 170 and 210\n",
    "        # Basically, from 170 cm, the transformation continues to change in the same way as it did between 130 cm and 170 cm\n",
    "        H_210 =  H_170 + (H_170 - H_130)\n",
    "        alpha = (z - 170) / 40 # /40 to normalize the alpha value (170 < z <= 210)\n",
    "        return (1 - alpha) * H_170 + alpha * H_210\n",
    "\n",
    "# Project a point on the floor, given its 3D coordinates\n",
    "def project_point(point, z):\n",
    "    H = interpolate_homography(z, H_0, H_130, H_170)\n",
    "    point_homogeneous = np.hstack([point, 1])\n",
    "    projected_point_homogeneous = H @ point_homogeneous\n",
    "    projected_point = projected_point_homogeneous[:2] / projected_point_homogeneous[2]\n",
    "    return projected_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Small example to test the homography interpolation\n",
    "image = cv2.imread(image_path)\n",
    "\n",
    "def project_points(points, H):\n",
    "    points_homogeneous = np.hstack([points[:, :2], np.ones((points.shape[0], 1))])\n",
    "    projected_points_homogeneous = (H @ points_homogeneous.T).T\n",
    "    projected_points = projected_points_homogeneous[:, :2] / projected_points_homogeneous[:, 2][:, np.newaxis]\n",
    "    return projected_points\n",
    "\n",
    "cv2.namedWindow('Projection Points', cv2.WINDOW_NORMAL)\n",
    "cv2.imshow('Projection Points', image)\n",
    "cv2.resizeWindow('Projection Points', 864, 648)\n",
    "\n",
    "# Draw the points at 170 cm\n",
    "for point in points_170.astype(int):\n",
    "    cv2.circle(image, tuple(point[:2]), 10, (255, 0, 0), -1)\n",
    "\n",
    "def on_trackbar(z):\n",
    "    z = int(z)\n",
    "    H = interpolate_homography(z, H_0, H_130, H_170)\n",
    "    projected_points = project_points(points_170, H)\n",
    "    image_with_points = image.copy()\n",
    "    for point in projected_points.astype(int):\n",
    "        cv2.circle(image_with_points, tuple(point), 10, (0, 0, 255), -1)\n",
    "    cv2.imshow('Projection Points', image_with_points)\n",
    "\n",
    "cv2.createTrackbar('Height', 'Projection Points', 170, 210, on_trackbar)\n",
    "\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "baslar_file = dataset_path + '\\\\solo_experiment\\\\7__40069753__20240625_144414557.mp4'\n",
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "height = 180\n",
    "projected_points = []\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(frame)\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            projected_point = project_point(point, height)\n",
    "            projected_points.append(projected_point)\n",
    "\n",
    "            cv2.circle(frame, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for projected_point in projected_points:\n",
    "            point = projected_point.astype(int)\n",
    "            cv2.circle(frame, tuple(point), 10, (0, 255, 0), -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Estimate exit times"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Estimate heights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream ended\n",
      "Total number of frames:  916\n",
      "{17: 1.8451363312547855}\n"
     ]
    }
   ],
   "source": [
    "camera_height = 2.568 # meters, ground truth\n",
    "\n",
    "bag_file = dataset_path + '\\\\solo_experiment\\\\rec.bag'\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(bag_file)\n",
    "num_frames = 0\n",
    "\n",
    "# Data structure to store the a list of height for every aruco marker id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        depth_frame = post_process_depth_frame(aligned_depth_frame)\n",
    "\n",
    "        # Convert color frame to image\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Calculate the distance using the center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "            distance = depth_frame.get_distance(x, y)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "\n",
    "            # Store the height for the aruco marker id\n",
    "            if not id in heights:\n",
    "                heights[id] = []\n",
    "                \n",
    "            heights[id].append(height)\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco marker id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the baslar video file\n",
    "baslar_file = dataset_path + '\\\\solo_experiment\\\\7__40069753__20240625_144414557.mp4'\n",
    "cap = cv2.VideoCapture(baslar_file)\n",
    "wait_key = 1\n",
    "num_frames = 0\n",
    "\n",
    "# Dictionary to store the projected points for each aruco marker id, and the color for each id, only for visualization\n",
    "projected_points = {}\n",
    "color_points = {}\n",
    "for id, _ in heights.items():\n",
    "    color_points[id] = (np.random.randint(0, 255), np.random.randint(0, 255), np.random.randint(0, 255))\n",
    "\n",
    "# Data structure to store the enter and exit frame for each aruco marker id\n",
    "enter_frame = {}\n",
    "exit_frame = {}\n",
    "for id, _ in heights.items():\n",
    "    enter_frame[id] = -1\n",
    "    exit_frame[id] = -1\n",
    "\n",
    "# Y coordinate of the \"exit line\"\n",
    "y_exit = 1130\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if ret:\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(frame)\n",
    "        cv2.aruco.drawDetectedMarkers(frame, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            id = ids[k][0]\n",
    "            c = corners[k][0]\n",
    "\n",
    "            # Center of the aruco marker\n",
    "            x = int(c[:, 0].sum() / 4)\n",
    "            y = int(c[:, 1].sum() / 4)\n",
    "\n",
    "            # Project the point on the floor\n",
    "            point = (x, y)\n",
    "            height = mean_heights[id] * 100 # cm\n",
    "            projected_point = project_point(point, height)\n",
    "\n",
    "            # Store the projected point for the aruco marker id\n",
    "            if not id in projected_points:\n",
    "                projected_points[id] = []\n",
    "                \n",
    "            projected_points[id].append(projected_point)\n",
    "\n",
    "            # Update the enter and exit frame\n",
    "            if enter_frame[id] == -1:\n",
    "                enter_frame[id] = num_frames\n",
    "\n",
    "            if exit_frame[id] == -1 and y > y_exit:\n",
    "                exit_frame[id] = num_frames\n",
    "\n",
    "            # Draw a circle at the center of the aruco marker\n",
    "            cv2.circle(frame, point, 10, (0, 0, 255), -1)\n",
    "        \n",
    "        # Draw all the projected points to get the trajectory\n",
    "        for id, points in projected_points.items():\n",
    "            for projected_point in points:\n",
    "                point = projected_point.astype(int)\n",
    "                if point[1] < y_exit:\n",
    "                    cv2.circle(frame, tuple(point), 10, color_points[id], -1)\n",
    "\n",
    "        cv2.namedWindow('Frame', cv2.WINDOW_NORMAL)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        cv2.resizeWindow('Frame', 864, 648)\n",
    "        \n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "    else:\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter frames:  {17: 183}\n",
      "Exit frames:  {17: 210}\n",
      "ID: 17 Exit time: 0.90s\n"
     ]
    }
   ],
   "source": [
    "print(\"Enter frames: \", enter_frame)\n",
    "print(\"Exit frames: \", exit_frame)\n",
    "\n",
    "# Compute the difference between the enter and exit frame, to estimate the exit time for each id\n",
    "exit_time = {}\n",
    "fps = 30\n",
    "for id, enter in enter_frame.items():\n",
    "    exit = exit_frame[id]\n",
    "    if enter != -1 and exit != -1:\n",
    "        exit_time[id] = (exit - enter) / fps\n",
    "\n",
    "# Print exit times in seconds\n",
    "for id, time in exit_time.items():\n",
    "    print(\"ID: {} Exit time: {:.2f}s\".format(id, time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attemping to enhance Aruco codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' camera_height = 2.65 # meters => NOT SURE ABOUT THIS VALUE\\n\\npipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\\nnum_frames = 0\\nwait_key = 1\\n\\n# Empty the distance_images folder\\nfiles = glob.glob(\\'distance_images/*\\')\\nfor f in files:\\n    os.remove(f)\\n\\n# Data structure to store the a list of height for every aruco code id\\nheights = {}\\n\\ntry:\\n    while True:\\n        frames = pipe.wait_for_frames()\\n\\n        # Align the depth frame to color frame so they have the same shape\\n        aligned_depth, aligned_color = align_frames(frames)\\n        color_image = np.asanyarray(aligned_color.get_data())\\n        depth_color_frame = colorizer.colorize(aligned_depth)\\n        depth_color_image = np.asanyarray(depth_color_frame.get_data())\\n\\n        # Apply the mask to the depth image\\n        depth_color_image = cv2.bitwise_and(depth_color_image, depth_color_image, mask=mask)\\n\\n        # Convert the image to HSV color space\\n        hsv_image = cv2.cvtColor(depth_color_image, cv2.COLOR_BGR2HSV)\\n\\n        # Define the range for red color in HSV\\n        # Note: Red can appear in two ranges in HSV\\n        lower_red1 = np.array([0, 70, 50])\\n        upper_red1 = np.array([10, 255, 255])\\n        lower_red2 = np.array([170, 70, 50])\\n        upper_red2 = np.array([180, 255, 255])\\n\\n        # Create masks for red color\\n        red_mask1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\\n        red_mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\\n\\n        # Combine the two masks\\n        red_mask = cv2.bitwise_or(red_mask1, red_mask2)\\n\\n        # Apply the mask to the original image\\n        result = cv2.bitwise_and(depth_color_image, depth_color_image, mask=red_mask)\\n\\n        # Convert the result to grayscale and apply thresholding\\n        gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\\n        _, thresh_result = cv2.threshold(gray_result, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\\n\\n        # Apply the closing morphological operation to fill in holes\\n        kernel = np.ones((30, 30), np.uint8)\\n        closed_result = cv2.morphologyEx(thresh_result, cv2.MORPH_CLOSE, kernel)\\n\\n        # Find contours of the red square\\n        contours, _ = cv2.findContours(closed_result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\\n\\n        # Create a mask to keep only the aruco code in the color image\\n        for contour in contours:\\n            area = cv2.contourArea(contour)\\n            if area > 100*100 and area < 200*200:\\n                x, y, w, h = cv2.boundingRect(contour)  \\n                arcuo_code_mask = np.zeros(frame_shape, dtype=np.uint8)\\n                \\n                # Expand the bounding box by 50 pixels\\n                sz = (w + h) // 2\\n                x1 = max(0, x - 25)\\n                y1 = max(0, y - 25)\\n                x2 = min(640, x + sz + 25)\\n                y2 = min(480, y + sz + 25)\\n                cv2.rectangle(arcuo_code_mask, (x1, y1), (x2, y2), (255, 255, 255), -1)\\n                color_image = cv2.bitwise_and(color_image, color_image, mask=arcuo_code_mask)\\n\\n                cv2.imwrite(\"color_images\\\\motion_blur_mask\\\\motion_blur\" + str(num_frames) + \".png\", color_image)\\n\\n        cv2.imshow(\"Output\", color_image)\\n        key = cv2.waitKey(wait_key)\\n\\n        # Press esc close the image window\\n        if key == 27:\\n            break\\n\\n        # Press d to view the video frame by frame\\n        if key == ord(\\'d\\'):\\n            if wait_key == 0:\\n                wait_key = 1\\n            else:\\n                wait_key = 0\\n\\n        num_frames += 1\\n           \\n# Catch exception if the stream is ended\\nexcept RuntimeError:\\n    print(\"Stream ended\")\\n        \\nfinally:\\n    # Stop streaming\\n    cv2.destroyAllWindows()\\n    pipe.stop() '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" camera_height = 2.65 # meters => NOT SURE ABOUT THIS VALUE\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Empty the distance_images folder\n",
    "files = glob.glob('distance_images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Data structure to store the a list of height for every aruco code id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame so they have the same shape\n",
    "        aligned_depth, aligned_color = align_frames(frames)\n",
    "        color_image = np.asanyarray(aligned_color.get_data())\n",
    "        depth_color_frame = colorizer.colorize(aligned_depth)\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "\n",
    "        # Apply the mask to the depth image\n",
    "        depth_color_image = cv2.bitwise_and(depth_color_image, depth_color_image, mask=mask)\n",
    "\n",
    "        # Convert the image to HSV color space\n",
    "        hsv_image = cv2.cvtColor(depth_color_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Define the range for red color in HSV\n",
    "        # Note: Red can appear in two ranges in HSV\n",
    "        lower_red1 = np.array([0, 70, 50])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([170, 70, 50])\n",
    "        upper_red2 = np.array([180, 255, 255])\n",
    "\n",
    "        # Create masks for red color\n",
    "        red_mask1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "        red_mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "\n",
    "        # Combine the two masks\n",
    "        red_mask = cv2.bitwise_or(red_mask1, red_mask2)\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        result = cv2.bitwise_and(depth_color_image, depth_color_image, mask=red_mask)\n",
    "\n",
    "        # Convert the result to grayscale and apply thresholding\n",
    "        gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh_result = cv2.threshold(gray_result, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Apply the closing morphological operation to fill in holes\n",
    "        kernel = np.ones((30, 30), np.uint8)\n",
    "        closed_result = cv2.morphologyEx(thresh_result, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours of the red square\n",
    "        contours, _ = cv2.findContours(closed_result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Create a mask to keep only the aruco code in the color image\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 100*100 and area < 200*200:\n",
    "                x, y, w, h = cv2.boundingRect(contour)  \n",
    "                arcuo_code_mask = np.zeros(frame_shape, dtype=np.uint8)\n",
    "                \n",
    "                # Expand the bounding box by 50 pixels\n",
    "                sz = (w + h) // 2\n",
    "                x1 = max(0, x - 25)\n",
    "                y1 = max(0, y - 25)\n",
    "                x2 = min(640, x + sz + 25)\n",
    "                y2 = min(480, y + sz + 25)\n",
    "                cv2.rectangle(arcuo_code_mask, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
    "                color_image = cv2.bitwise_and(color_image, color_image, mask=arcuo_code_mask)\n",
    "\n",
    "                cv2.imwrite(\"color_images\\motion_blur_mask\\motion_blur\" + str(num_frames) + \".png\", color_image)\n",
    "\n",
    "        cv2.imshow(\"Output\", color_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop() \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' image = cv2.imread(\"color_images\\\\motion_blur\\\\motion_blur1229.png\", cv2.IMREAD_GRAYSCALE)\\ncorners, ids, rejected = detector.detectMarkers(image)\\noutput_image = image.copy()\\nif corners is not None:\\n    print(\"Detected markers: \", len(corners))\\n\\ncv2.aruco.drawDetectedMarkers(output_image, corners, ids)\\ncv2.imshow(\"Output\", output_image)\\n\\n# Show the marker id image\\nmarker_id = ids[0][0]\\nprint(\"Marker ID: \", marker_id)\\nmarker_size = 200  # Taille de l\\'image générée\\naruco_marker_image = cv2.aruco.generateImageMarker(aruco_dict, marker_id, marker_size)\\ncv2.imshow(\"Marker\", aruco_marker_image)\\ncv2.waitKey(0)\\ncv2.destroyAllWindows() '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\" image = cv2.imread(\"color_images\\motion_blur\\motion_blur1229.png\", cv2.IMREAD_GRAYSCALE)\n",
    "corners, ids, rejected = detector.detectMarkers(image)\n",
    "output_image = image.copy()\n",
    "if corners is not None:\n",
    "    print(\"Detected markers: \", len(corners))\n",
    "\n",
    "cv2.aruco.drawDetectedMarkers(output_image, corners, ids)\n",
    "cv2.imshow(\"Output\", output_image)\n",
    "\n",
    "# Show the marker id image\n",
    "marker_id = ids[0][0]\n",
    "print(\"Marker ID: \", marker_id)\n",
    "marker_size = 200  # Taille de l'image générée\n",
    "aruco_marker_image = cv2.aruco.generateImageMarker(aruco_dict, marker_id, marker_size)\n",
    "cv2.imshow(\"Marker\", aruco_marker_image)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows() \"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
