{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import cv2\n",
    "import pyrealsense2 as rs\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_bag_file(file_name):\n",
    "    pipe = rs.pipeline()\n",
    "    cfg = rs.config()\n",
    "    cfg.enable_device_from_file(file_name, repeat_playback=False)\n",
    "    profile = pipe.start(cfg)\n",
    "    playback = profile.get_device().as_playback()\n",
    "    playback.set_real_time(False) # False: no frame drop\n",
    "    \n",
    "    # Get the frame shape of the color sensor\n",
    "    frames = pipe.wait_for_frames()\n",
    "    color_frame = frames.get_color_frame()\n",
    "    frame_shape = (color_frame.get_height(), color_frame.get_width())\n",
    "\n",
    "    return pipe, cfg, profile, playback, frame_shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video duration:  0:00:11.976164\n",
      "Frame shape:  (480, 848)\n"
     ]
    }
   ],
   "source": [
    "# Read the bag file\n",
    "# file_name = \"..\\\\Dataset\\\\_realsense\\\\Data_realsense1\\\\20240322_123504.bag\"\n",
    "#file_name = \"..\\\\..\\\\Dataset\\\\tests\\\\20240620_113148.bag\"\n",
    "file_name = \"..\\\\bag_records\\\\record.bag\"\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\n",
    "duration = playback.get_duration()\n",
    "print(\"Video duration: \", duration)\n",
    "print(\"Frame shape: \", frame_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def post_process_depth_frame(depth_frame):\n",
    "    # Post processing possible only on the depth_frame\n",
    "    assert (depth_frame.is_depth_frame())\n",
    "\n",
    "    decimation_magnitude = 1.0\n",
    "    spatial_magnitude = 2.0\n",
    "    spatial_smooth_alpha = 0.5\n",
    "    spatial_smooth_delta = 20\n",
    "    temporal_smooth_alpha = 0.4\n",
    "    temporal_smooth_delta = 20\n",
    "\n",
    "    # Available filters and control options for the filters\n",
    "    decimation_filter = rs.decimation_filter()\n",
    "    depth_to_disparity = rs.disparity_transform(True)\n",
    "    spatial_filter = rs.spatial_filter()\n",
    "    temporal_filter = rs.temporal_filter()\n",
    "    hole_filling = rs.hole_filling_filter(1)\n",
    "    # 0 - fill_from_left - Use the value from the left neighbor pixel to fill the hole\n",
    "    # 1 - farest_from_around - Use the value from the neighboring pixel which is furthest away from the sensor\n",
    "    # 2 - nearest_from_around - Use the value from the neighboring pixel closest to the sensor\n",
    "    disparity_to_depth = rs.disparity_transform(False)\n",
    "\n",
    "    # Apply the control parameters for the filter\n",
    "    decimation_filter.set_option(rs.option.filter_magnitude, decimation_magnitude)\n",
    "\n",
    "    spatial_filter.set_option(rs.option.filter_magnitude, spatial_magnitude)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_alpha, spatial_smooth_alpha)\n",
    "    spatial_filter.set_option(rs.option.filter_smooth_delta, spatial_smooth_delta)\n",
    "\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_alpha, temporal_smooth_alpha)\n",
    "    temporal_filter.set_option(rs.option.filter_smooth_delta, temporal_smooth_delta)\n",
    "\n",
    "    # Apply the filters\n",
    "    # Post processing order : https://dev.intelrealsense.com/docs/post-processing-filters\n",
    "    # Depth Frame >> Decimation Filter >> Depth2Disparity Transform >> Spatial Filter \n",
    "    # >> Temporal Filter >> Disparity2Depth Transform >> Hole Filling Filter >> Filtered Depth\n",
    "\n",
    "    filtered_frame = decimation_filter.process(depth_frame)\n",
    "    filtered_frame = depth_to_disparity.process(filtered_frame)\n",
    "    filtered_frame = spatial_filter.process(filtered_frame)\n",
    "    filtered_frame = temporal_filter.process(filtered_frame)\n",
    "    filtered_frame = disparity_to_depth.process(filtered_frame)\n",
    "    filtered_frame = hole_filling.process(filtered_frame)\n",
    "    \n",
    "    return filtered_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of frames:  103\n"
     ]
    }
   ],
   "source": [
    "colorizer = rs.colorizer()\n",
    "align = rs.align(rs.stream.color)\n",
    "\n",
    "# Read the full stream\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Post process is not included in the BAG file, so we need to apply it\n",
    "        depth_frame = post_process_depth_frame(aligned_depth_frame)\n",
    "\n",
    "        # Colorize the depth frame\n",
    "        depth_color_frame = colorizer.colorize(depth_frame)\n",
    "\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "\n",
    "        cv2.imshow(\"Color Image\", color_image)\n",
    "        cv2.imshow(\"Depth Image\", depth_color_image)\n",
    "\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press s to write the color_image on disk\n",
    "        if key == ord('s'):\n",
    "            cv2.imwrite(\".\\color_images\\images\\color_\" + str(num_frames) + \".png\", color_image)\n",
    "            cv2.imwrite(\".\\depth_images\\depth_\" + str(num_frames) + \".png\", depth_color_image)\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "\n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()\n",
    "\n",
    "print(\"Total number of frames: \", num_frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract height of the participants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Create a mask to keep ony the ROI, under the camera\n",
    "# mask = np.zeros(frame_shape, dtype=np.uint8)\n",
    "# center_y, center_x = frame_shape[0] // 2, frame_shape[1] // 2\n",
    "\n",
    "# # Mask size\n",
    "# half_height = frame_shape[0] // 3\n",
    "# half_width = frame_shape[1] // 3\n",
    "# top_left = (center_x - half_width, center_y - half_height)\n",
    "# bottom_right = (center_x + half_width, center_y + half_height)\n",
    "\n",
    "# cv2.rectangle(mask, top_left, bottom_right, 255, thickness=-1)\n",
    "\n",
    "# plt.imshow(mask, cmap='gray')\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup the aruco detector\n",
    "aruco_dict = cv2.aruco.getPredefinedDictionary(cv2.aruco.DICT_ARUCO_MIP_36h12)\n",
    "aruco_params = cv2.aruco.DetectorParameters()\n",
    "\n",
    "aruco_params.polygonalApproxAccuracyRate = 0.05\n",
    "aruco_params.errorCorrectionRate = 0.2\n",
    "\n",
    "detector = cv2.aruco.ArucoDetector(aruco_dict, aruco_params) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sign(p1, p2, p3):\n",
    "    return (p1[0] - p3[0]) * (p2[1] - p3[1]) - (p2[0] - p3[0]) * (p1[1] - p3[1])\n",
    "\n",
    "def is_inside(p, a, b, c, d):\n",
    "    b1 = sign(p, a, b) < 0.0\n",
    "    b2 = sign(p, b, c) < 0.0\n",
    "    b3 = sign(p, c, d) < 0.0\n",
    "    b4 = sign(p, d, a) < 0.0\n",
    "    return ((b1 == b2) and (b2 == b3) and (b3 == b4))\n",
    "\n",
    "def is_point_in_square(A, B, C, D, P):\n",
    "    return is_inside(P, A, B, C, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_height = 2.49 # meters, ground truth\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Empty the distance_images folder\n",
    "files = glob.glob('distance_images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Data structure to store the a list of height for every aruco code id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Get frameset of color and depth\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame\n",
    "        aligned_frames = align.process(frames)\n",
    "\n",
    "        # Get aligned frames\n",
    "        aligned_depth_frame = aligned_frames.get_depth_frame()\n",
    "        color_frame = aligned_frames.get_color_frame()\n",
    "\n",
    "        # Validate that both frames are valid\n",
    "        if not aligned_depth_frame or not color_frame:\n",
    "            continue\n",
    "\n",
    "        # Apply the mask to the color image\n",
    "        color_image = np.asanyarray(color_frame.get_data())\n",
    "        color_image = cv2.bitwise_and(color_image, color_image, mask=mask)\n",
    "\n",
    "        # Detect the aruco markers\n",
    "        corners, ids, rejected = detector.detectMarkers(color_image)\n",
    "        output_image = color_image.copy()\n",
    "        cv2.aruco.drawDetectedMarkers(output_image, corners, ids)\n",
    "\n",
    "        for k in range(len(corners)):\n",
    "            c = corners[k][0]\n",
    "            A = c[0]\n",
    "            B = c[1]\n",
    "            C = c[2]\n",
    "            D = c[3]\n",
    "            distances = []\n",
    "\n",
    "            # Pick 100 random points inside the aruco code square to calculate the average distance\n",
    "            min_x = int(min(A[0], B[0], C[0], D[0]))\n",
    "            max_x = int(max(A[0], B[0], C[0], D[0]))\n",
    "            min_y = int(min(A[1], B[1], C[1], D[1]))\n",
    "            max_y = int(max(A[1], B[1], C[1], D[1]))\n",
    "\n",
    "            num_points = 0\n",
    "            while num_points < 100:\n",
    "                x = np.random.randint(min_x, max_x + 1) \n",
    "                y = np.random.randint(min_y, max_y + 1)\n",
    "\n",
    "                if is_point_in_square(A, B, C, D, (x, y)):\n",
    "                    # Draw the point on the image\n",
    "                    cv2.circle(output_image, (x, y), 2, (0, 255, 0), -1)\n",
    "                    z = aligned_depth_frame.get_distance(x, y)\n",
    "                    distances.append(z)\n",
    "                    num_points += 1\n",
    "\n",
    "            # Calculate the average distance\n",
    "            distance = np.mean(distances)\n",
    "\n",
    "            # Calculate the height\n",
    "            height = camera_height - distance\n",
    "            cv2.putText(output_image, \"Height: {:.2f} m\".format(height), (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "            \n",
    "            # Store the height for the aruco code id\n",
    "            if not ids[k][0] in heights:\n",
    "                heights[ids[k][0]] = []\n",
    "                \n",
    "            heights[ids[k][0]].append(height)\n",
    "\n",
    "        cv2.imshow(\"Output\", output_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{24: 1.7995207289669477}\n"
     ]
    }
   ],
   "source": [
    "def remove_outliers(data, m=2):\n",
    "    # Z-score method\n",
    "    data = np.array(data)\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    filtered_data = [x for x in data if abs(x - mean) / std < m]\n",
    "    return filtered_data\n",
    "\n",
    "def calculate_mean_height(heights):\n",
    "    mean_heights = {}\n",
    "\n",
    "    for key, height_list in heights.items():\n",
    "        filtered_heights = remove_outliers(height_list)\n",
    "        if filtered_heights:\n",
    "            mean_heights[key] = np.mean(filtered_heights)\n",
    "\n",
    "    return mean_heights\n",
    "\n",
    "# Remove outliers and calculate the mean height for each aruco code id\n",
    "mean_heights = calculate_mean_height(heights)\n",
    "print(mean_heights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attemping to enhance Aruco codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "camera_height = 2.65 # meters => NOT SURE ABOUT THIS VALUE\n",
    "\n",
    "pipe, cfg, profile, playback, frame_shape = read_bag_file(file_name)\n",
    "num_frames = 0\n",
    "wait_key = 1\n",
    "\n",
    "# Empty the distance_images folder\n",
    "files = glob.glob('distance_images/*')\n",
    "for f in files:\n",
    "    os.remove(f)\n",
    "\n",
    "# Data structure to store the a list of height for every aruco code id\n",
    "heights = {}\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        frames = pipe.wait_for_frames()\n",
    "\n",
    "        # Align the depth frame to color frame so they have the same shape\n",
    "        aligned_depth, aligned_color = align_frames(frames)\n",
    "        color_image = np.asanyarray(aligned_color.get_data())\n",
    "        depth_color_frame = colorizer.colorize(aligned_depth)\n",
    "        depth_color_image = np.asanyarray(depth_color_frame.get_data())\n",
    "\n",
    "        # Apply the mask to the depth image\n",
    "        depth_color_image = cv2.bitwise_and(depth_color_image, depth_color_image, mask=mask)\n",
    "\n",
    "        # Convert the image to HSV color space\n",
    "        hsv_image = cv2.cvtColor(depth_color_image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "        # Define the range for red color in HSV\n",
    "        # Note: Red can appear in two ranges in HSV\n",
    "        lower_red1 = np.array([0, 70, 50])\n",
    "        upper_red1 = np.array([10, 255, 255])\n",
    "        lower_red2 = np.array([170, 70, 50])\n",
    "        upper_red2 = np.array([180, 255, 255])\n",
    "\n",
    "        # Create masks for red color\n",
    "        red_mask1 = cv2.inRange(hsv_image, lower_red1, upper_red1)\n",
    "        red_mask2 = cv2.inRange(hsv_image, lower_red2, upper_red2)\n",
    "\n",
    "        # Combine the two masks\n",
    "        red_mask = cv2.bitwise_or(red_mask1, red_mask2)\n",
    "\n",
    "        # Apply the mask to the original image\n",
    "        result = cv2.bitwise_and(depth_color_image, depth_color_image, mask=red_mask)\n",
    "\n",
    "        # Convert the result to grayscale and apply thresholding\n",
    "        gray_result = cv2.cvtColor(result, cv2.COLOR_BGR2GRAY)\n",
    "        _, thresh_result = cv2.threshold(gray_result, 0, 255, cv2.THRESH_BINARY + cv2.THRESH_OTSU)\n",
    "\n",
    "        # Apply the closing morphological operation to fill in holes\n",
    "        kernel = np.ones((30, 30), np.uint8)\n",
    "        closed_result = cv2.morphologyEx(thresh_result, cv2.MORPH_CLOSE, kernel)\n",
    "\n",
    "        # Find contours of the red square\n",
    "        contours, _ = cv2.findContours(closed_result, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "\n",
    "        # Create a mask to keep only the aruco code in the color image\n",
    "        for contour in contours:\n",
    "            area = cv2.contourArea(contour)\n",
    "            if area > 100*100 and area < 200*200:\n",
    "                x, y, w, h = cv2.boundingRect(contour)  \n",
    "                arcuo_code_mask = np.zeros(frame_shape, dtype=np.uint8)\n",
    "                \n",
    "                # Expand the bounding box by 50 pixels\n",
    "                sz = (w + h) // 2\n",
    "                x1 = max(0, x - 25)\n",
    "                y1 = max(0, y - 25)\n",
    "                x2 = min(640, x + sz + 25)\n",
    "                y2 = min(480, y + sz + 25)\n",
    "                cv2.rectangle(arcuo_code_mask, (x1, y1), (x2, y2), (255, 255, 255), -1)\n",
    "                color_image = cv2.bitwise_and(color_image, color_image, mask=arcuo_code_mask)\n",
    "\n",
    "                cv2.imwrite(\"color_images\\motion_blur_mask\\motion_blur\" + str(num_frames) + \".png\", color_image)\n",
    "\n",
    "        cv2.imshow(\"Output\", color_image)\n",
    "        key = cv2.waitKey(wait_key)\n",
    "\n",
    "        # Press esc close the image window\n",
    "        if key == 27:\n",
    "            break\n",
    "\n",
    "        # Press d to view the video frame by frame\n",
    "        if key == ord('d'):\n",
    "            if wait_key == 0:\n",
    "                wait_key = 1\n",
    "            else:\n",
    "                wait_key = 0\n",
    "\n",
    "        num_frames += 1\n",
    "           \n",
    "# Catch exception if the stream is ended\n",
    "except RuntimeError:\n",
    "    print(\"Stream ended\")\n",
    "        \n",
    "finally:\n",
    "    # Stop streaming\n",
    "    cv2.destroyAllWindows()\n",
    "    pipe.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# image = cv2.imread(\"color_images\\motion_blur\\motion_blur1229.png\", cv2.IMREAD_GRAYSCALE)\n",
    "# corners, ids, rejected = detector.detectMarkers(image)\n",
    "# output_image = image.copy()\n",
    "# if corners is not None:\n",
    "#     print(\"Detected markers: \", len(corners))\n",
    "\n",
    "# cv2.aruco.drawDetectedMarkers(output_image, corners, ids)\n",
    "# cv2.imshow(\"Output\", output_image)\n",
    "\n",
    "# # Show the marker id image\n",
    "# marker_id = ids[0][0]\n",
    "# print(\"Marker ID: \", marker_id)\n",
    "# marker_size = 200  # Taille de l'image générée\n",
    "# aruco_marker_image = cv2.aruco.generateImageMarker(aruco_dict, marker_id, marker_size)\n",
    "# cv2.imshow(\"Marker\", aruco_marker_image)\n",
    "# cv2.waitKey(0)\n",
    "# cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
